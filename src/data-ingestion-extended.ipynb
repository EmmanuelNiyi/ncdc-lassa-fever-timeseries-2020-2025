{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":13947133,"sourceType":"datasetVersion","datasetId":8842247}],"dockerImageVersionId":31192,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/emmanuelniyioriolowo/data-ingestion-extended?scriptVersionId=283336803\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"## Initial Setup","metadata":{}},{"cell_type":"code","source":"# Install required PDF processing libraries\n!pip install reportlab\n!pip install fpdf\n!pip install pdfplumber","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-02T10:33:21.509291Z","iopub.execute_input":"2025-12-02T10:33:21.509661Z","iopub.status.idle":"2025-12-02T10:33:41.545001Z","shell.execute_reply.started":"2025-12-02T10:33:21.509627Z","shell.execute_reply":"2025-12-02T10:33:41.543811Z"},"jupyter":{"source_hidden":true}},"outputs":[{"name":"stdout","text":"Collecting reportlab\n  Downloading reportlab-4.4.5-py3-none-any.whl.metadata (1.7 kB)\nRequirement already satisfied: pillow>=9.0.0 in /usr/local/lib/python3.11/dist-packages (from reportlab) (11.3.0)\nRequirement already satisfied: charset-normalizer in /usr/local/lib/python3.11/dist-packages (from reportlab) (3.4.4)\nDownloading reportlab-4.4.5-py3-none-any.whl (2.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m35.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: reportlab\nSuccessfully installed reportlab-4.4.5\nCollecting fpdf\n  Downloading fpdf-1.7.2.tar.gz (39 kB)\n  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\nBuilding wheels for collected packages: fpdf\n  Building wheel for fpdf (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for fpdf: filename=fpdf-1.7.2-py2.py3-none-any.whl size=40704 sha256=2b395aa181897d55d5507ba84ccb921fa3aac0722b4ec7af87990bdc7fc49cc7\n  Stored in directory: /root/.cache/pip/wheels/65/4f/66/bbda9866da446a72e206d6484cd97381cbc7859a7068541c36\nSuccessfully built fpdf\nInstalling collected packages: fpdf\nSuccessfully installed fpdf-1.7.2\nCollecting pdfplumber\n  Downloading pdfplumber-0.11.8-py3-none-any.whl.metadata (43 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.6/43.6 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting pdfminer.six==20251107 (from pdfplumber)\n  Downloading pdfminer_six-20251107-py3-none-any.whl.metadata (4.2 kB)\nRequirement already satisfied: Pillow>=9.1 in /usr/local/lib/python3.11/dist-packages (from pdfplumber) (11.3.0)\nCollecting pypdfium2>=4.18.0 (from pdfplumber)\n  Downloading pypdfium2-5.1.0-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (67 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.7/67.7 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: charset-normalizer>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from pdfminer.six==20251107->pdfplumber) (3.4.4)\nRequirement already satisfied: cryptography>=36.0.0 in /usr/local/lib/python3.11/dist-packages (from pdfminer.six==20251107->pdfplumber) (46.0.3)\nRequirement already satisfied: cffi>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from cryptography>=36.0.0->pdfminer.six==20251107->pdfplumber) (2.0.0)\nRequirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=2.0.0->cryptography>=36.0.0->pdfminer.six==20251107->pdfplumber) (2.23)\nDownloading pdfplumber-0.11.8-py3-none-any.whl (60 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.0/60.0 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading pdfminer_six-20251107-py3-none-any.whl (5.6 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m92.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hDownloading pypdfium2-5.1.0-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m71.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: pypdfium2, pdfminer.six, pdfplumber\nSuccessfully installed pdfminer.six-20251107 pdfplumber-0.11.8 pypdfium2-5.1.0\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"# Standard library imports\nimport os\nimport re\nimport datetime as dt\nimport shutil\nfrom urllib.parse import urljoin\n\n# Third-party libraries\nimport requests\nimport pdfplumber\nimport numpy as np\nimport pandas as pd\nfrom bs4 import BeautifulSoup\nfrom reportlab.pdfgen import canvas\nfrom fpdf import FPDF\nfrom typing import Optional, Dict, Any, Tuple","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-02T10:33:41.547453Z","iopub.execute_input":"2025-12-02T10:33:41.547799Z","iopub.status.idle":"2025-12-02T10:33:42.592023Z","shell.execute_reply.started":"2025-12-02T10:33:41.547767Z","shell.execute_reply":"2025-12-02T10:33:42.591258Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"# Base configuration for dataset extraction\nBASE_URL = \"https://ncdc.gov.ng\"\nHTML_FILE = \"/kaggle/input/ncdc-table/ncdc_lassa_fever_html_table.html\"\nSAVE_DIR = \"/kaggle/working/ncdc_reports\"\n\n# Starting point for serial number and week tracking\nSTART_SN = 307      # Serial number for Week 1, 2020\nSTART_YEAR = 2020\nSTART_WEEK = 1\n\n# Make and ensure output directory exists\nos.makedirs(SAVE_DIR, exist_ok=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-02T10:33:42.592802Z","iopub.execute_input":"2025-12-02T10:33:42.593179Z","iopub.status.idle":"2025-12-02T10:33:42.599766Z","shell.execute_reply.started":"2025-12-02T10:33:42.593155Z","shell.execute_reply":"2025-12-02T10:33:42.598647Z"}},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":"## URL extraction from HTML file","metadata":{}},{"cell_type":"code","source":"def insert_missing_week_clean(pdf_data, after_sn, missing_week, year=None):\n    \"\"\"\n    Inserts a missing weekly report into a sequential (SN-sorted) list.\n    \n    The function:\n    - Locates the entry with serial number `after_sn`\n    - Inserts a placeholder record for `missing_week` directly after it\n    - Shifts all subsequent serial numbers by +1 to keep numbering consistent\n    \n    Args:\n        pdf_data (list): List of tuples -> (sn, title, url)\n        after_sn (int): Serial number after which the missing week is inserted\n        missing_week (int): Epi week number to insert\n        year (int, optional): Year for the missing entry title\n    \n    Returns:\n        list: Updated list with corrected SN sequence\n    \"\"\"\n    if year is None:\n        year = dt.datetime.now().year\n\n    new_data = []\n    inserted = False\n\n    for i, (sn, title, url) in enumerate(pdf_data):\n        # Append current entry before checking for insertion point\n        new_data.append((sn, title, url))\n\n        # Insert placeholder immediately after the target SN\n        if sn == after_sn and not inserted:\n            missing_title = (\n                f\"An update of Lassa fever outbreak in Nigeria for Week {missing_week}, {year}\"\n            )\n\n            # Add the missing week entry\n            new_data.append((after_sn + 1, missing_title, None))\n            inserted = True\n\n            # Shift all subsequent SN values by +1\n            for remaining_sn, remaining_title, remaining_url in pdf_data[i + 1:]:\n                new_data.append((remaining_sn + 1, remaining_title, remaining_url))\n            break\n\n    return new_data","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-02T10:33:42.600674Z","iopub.execute_input":"2025-12-02T10:33:42.600954Z","iopub.status.idle":"2025-12-02T10:33:42.621444Z","shell.execute_reply.started":"2025-12-02T10:33:42.60093Z","shell.execute_reply":"2025-12-02T10:33:42.620321Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"# Load saved HTML table from local file\nwith open(HTML_FILE, \"r\", encoding=\"utf-8\") as f:\n    soup = BeautifulSoup(f, \"html.parser\")  # parse HTML with BeautifulSoup\n\n# Get all table rows, skipping header\nrows = soup.find_all(\"tr\")[1:]  # skip header row\n\n# Initialize list to store serial number (SN) and PDF link info\npdf_data = []\n\n# Extract SN, title, and PDF URL from each row\nfor row in rows:\n    cols = row.find_all(\"td\")\n    if len(cols) >= 3:\n        sn = cols[0].get_text(strip=True)  # extract serial number\n        title = cols[1].get_text(strip=True)  # extract report title\n        link_tag = cols[2].find(\"a\", href=True)  # find anchor tag in third column\n        if link_tag:\n            pdf_url = urljoin(BASE_URL, link_tag['href'])  # construct full PDF URL\n            pdf_data.append((int(sn), title, pdf_url))\n\n# Sort the PDF data by serial number ascending\npdf_data.sort(key=lambda x: x[0])\n\n# Output total number of PDFs found\nprint(f\"Total PDFs found: {len(pdf_data)}\")\n\n# Preview relevant entries\n# for sn, title, url in pdf_data[:306]:\nfor sn, title, url in pdf_data[:9]:\n    print(f\"SN {sn} | Title: {title} | URL: {url}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-02T10:33:42.624419Z","iopub.execute_input":"2025-12-02T10:33:42.624748Z","iopub.status.idle":"2025-12-02T10:33:42.835078Z","shell.execute_reply.started":"2025-12-02T10:33:42.624724Z","shell.execute_reply":"2025-12-02T10:33:42.83396Z"}},"outputs":[{"name":"stdout","text":"Total PDFs found: 442\nSN 1 | Title: An update of Lassa fever outbreak in Nigeria for Week 46 | URL: https://ncdc.gov.ng/themes/common/files/sitreps/65b81d453825965fc88b202a61101c4f.pdf\nSN 2 | Title: An update of Lassa fever outbreak in Nigeria for Week 45 | URL: https://ncdc.gov.ng/themes/common/files/sitreps/471dd6384cd4f288167c0881efee40ee.pdf\nSN 3 | Title: An update of Lassa fever outbreak in Nigeria for Week 44 | URL: https://ncdc.gov.ng/themes/common/files/sitreps/9910b5860f5b9c377cf14a8992d6905b.pdf\nSN 4 | Title: An update of Lassa fever outbreak in Nigeria for Week 43 | URL: https://ncdc.gov.ng/themes/common/files/sitreps/55b33161bbfe39fd72039808875e5c45.pdf\nSN 5 | Title: An update of Lassa fever outbreak in Nigeria for Week 42 | URL: https://ncdc.gov.ng/themes/common/files/sitreps/91f559d691c7cd411f7058f1496eef6f.pdf\nSN 6 | Title: An update of Lassa fever outbreak in Nigeria for Week 41 | URL: https://ncdc.gov.ng/themes/common/files/sitreps/68f0ee9000f3084b74ed892be0d3f79f.pdf\nSN 7 | Title: An update of Lassa fever outbreak in Nigeria for Week 40 | URL: https://ncdc.gov.ng/themes/common/files/sitreps/ded728ef465f33739ad7928d327a2929.pdf\nSN 8 | Title: An update of Lassa fever outbreak in Nigeria for Week 39 | URL: https://ncdc.gov.ng/themes/common/files/sitreps/29888ec0c2dd10ca4cf0bef3814d0a81.pdf\nSN 9 | Title: An update of Lassa fever outbreak in Nigeria for Week 38 | URL: https://ncdc.gov.ng/themes/common/files/sitreps/bc2702d096dea7bcb5f8cb520aefba7c.pdf\n","output_type":"stream"}],"execution_count":6},{"cell_type":"markdown","source":"**On inspection of the URLS there is a missing issing upload at index 181 ----> 2022 W 22**","metadata":{}},{"cell_type":"code","source":"for sn, title, url in pdf_data[179:182]:\n    print(f\"SN {sn} | Title: {title} | URL: {url}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-02T10:33:42.835995Z","iopub.execute_input":"2025-12-02T10:33:42.836311Z","iopub.status.idle":"2025-12-02T10:33:42.84243Z","shell.execute_reply.started":"2025-12-02T10:33:42.836284Z","shell.execute_reply":"2025-12-02T10:33:42.841408Z"}},"outputs":[{"name":"stdout","text":"SN 180 | Title: An update of Lassa fever outbreak in Nigeria for Week 23 | URL: https://ncdc.gov.ng/themes/common/files/sitreps/415ea980abafa9fb2547cbcd0362004e.pdf\nSN 181 | Title: An update of Lassa fever outbreak in Nigeria for Week 21 | URL: https://ncdc.gov.ng/themes/common/files/sitreps/6ce166b15bffc6986d356448d48d6a34.pdf\nSN 182 | Title: An update of Lassa fever outbreak in Nigeria for Week 20 | URL: https://ncdc.gov.ng/themes/common/files/sitreps/e1e22accee2e5c3272b863084821ff16.pdf\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"# create a placeholder \npdf_data = insert_missing_week_clean(pdf_data, after_sn=180, missing_week=22)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-02T10:33:42.843294Z","iopub.execute_input":"2025-12-02T10:33:42.84355Z","iopub.status.idle":"2025-12-02T10:33:42.863659Z","shell.execute_reply.started":"2025-12-02T10:33:42.843528Z","shell.execute_reply":"2025-12-02T10:33:42.862462Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"for sn, title, url in pdf_data[178:183]:\n    print(f\"SN {sn} | Title: {title} | URL: {url}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-02T10:33:42.864755Z","iopub.execute_input":"2025-12-02T10:33:42.865096Z","iopub.status.idle":"2025-12-02T10:33:42.884672Z","shell.execute_reply.started":"2025-12-02T10:33:42.865071Z","shell.execute_reply":"2025-12-02T10:33:42.883671Z"}},"outputs":[{"name":"stdout","text":"SN 179 | Title: An update of Lassa fever outbreak in Nigeria for Week 24 | URL: https://ncdc.gov.ng/themes/common/files/sitreps/317f89c26656c4b549e07e84a720db5b.pdf\nSN 180 | Title: An update of Lassa fever outbreak in Nigeria for Week 23 | URL: https://ncdc.gov.ng/themes/common/files/sitreps/415ea980abafa9fb2547cbcd0362004e.pdf\nSN 181 | Title: An update of Lassa fever outbreak in Nigeria for Week 22, 2025 | URL: None\nSN 182 | Title: An update of Lassa fever outbreak in Nigeria for Week 21 | URL: https://ncdc.gov.ng/themes/common/files/sitreps/6ce166b15bffc6986d356448d48d6a34.pdf\nSN 183 | Title: An update of Lassa fever outbreak in Nigeria for Week 20 | URL: https://ncdc.gov.ng/themes/common/files/sitreps/e1e22accee2e5c3272b863084821ff16.pdf\n","output_type":"stream"}],"execution_count":9},{"cell_type":"markdown","source":"## PDF file download","metadata":{}},{"cell_type":"code","source":"# Helper functions for downloading PDFs\n\ndef has_53_weeks(year: int) -> bool:\n    \"\"\"\n    Check if a given year has 53 ISO weeks.\n    \n    Args:\n        year (int): Year to check.\n        \n    Returns:\n        bool: True if year has 53 weeks, else False.\n    \"\"\"\n    # ISO calendar: week number of Dec 31 determines if year has 53 weeks\n    return dt.date(year, 12, 31).isocalendar()[1] == 53\n\n\ndef download_pdf(url: str, filename: str) -> bool:\n    \"\"\"\n    Download a PDF from a URL and save it locally.\n    \n    Args:\n        url (str): URL of the PDF.\n        filename (str): Name to save the PDF as.\n        \n    Returns:\n        bool: True if download succeeds, False otherwise.\n    \"\"\"\n    try:\n        response = requests.get(url, timeout=30)  # fetch PDF\n        response.raise_for_status()  # raise error for bad status\n        with open(os.path.join(SAVE_DIR, filename), 'wb') as f:\n            f.write(response.content)  # save file\n        print(f\"✓ Downloaded: {filename}\")\n        return True\n    except Exception as e:\n        print(f\"✗ Failed to download {filename}: {e}\")\n        return False\n\n\ndef create_placeholder_pdf(filename: str, week_info: str):\n    \"\"\"\n    Create a placeholder PDF for weeks with missing reports.\n    \n    Args:\n        filename (str): Name to save the PDF as.\n        week_info (str): Week information to display in the PDF.\n    \"\"\"\n    pdf = FPDF()\n    pdf.add_page()\n    pdf.set_font(\"Arial\", size=12)\n    pdf.cell(200, 10, txt=\"Report Not Available\", ln=1, align='C')\n    pdf.cell(200, 10, txt=f\"Week {week_info}\", ln=1, align='C')\n    pdf.cell(200, 10, txt=\"Placeholder for missing report\", ln=1, align='C')\n    pdf.output(os.path.join(SAVE_DIR, filename))  # save PDF","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-02T10:33:42.885818Z","iopub.execute_input":"2025-12-02T10:33:42.886125Z","iopub.status.idle":"2025-12-02T10:33:42.906209Z","shell.execute_reply.started":"2025-12-02T10:33:42.886104Z","shell.execute_reply":"2025-12-02T10:33:42.904945Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"\"\"\"\nProcess PDFs for all serial numbers (SN) from START_SN downward.\nDownloads existing PDFs or creates placeholders if missing.\nCalculates the correct year and ISO week, accounting for 52/53-week years.\n\nFor faster execution and smaller repository size, the default configuration downloads only 10 reports.\nIf you require the complete dataset, you may uncomment the relevant lines in the ingestion script to enable full PDF download. \nUsers are encouraged to adjust this based on their computational needs and internet bandwidth.\n\"\"\"\n\n# Determine the actual minimum and maximum SN in the data\nmin_sn = min(sn for sn, _, _ in pdf_data)\nmax_sn = max(sn for sn, _, _ in pdf_data)\n\nprint(f\"SN range in data: {min_sn} to {max_sn}\")\nprint(f\"\\nProcessing SN range: {START_SN} to {min_sn}\")\n\nyear = START_YEAR\nweek = START_WEEK\n\n# Loop over SNs from START_SN downward\n# for sn in range(START_SN, 0, -1): # uncomment when not testing\nfor sn in range(START_SN, START_SN - 10, -1): # for testing\n    # Calculate week offset from START_SN\n\n    \n    # Adjust year and week if week exceeds total weeks in current year\n    weeks_in_year = 53 if has_53_weeks(year) else 52\n\n    # Construct filename for the PDF\n    filename = f\"{year}-W{week:02d}.pdf\"\n    \n    # Find PDF info for this SN\n    pdf_info = next((item for item in pdf_data if item[0] == sn), None)\n    \n    if pdf_info:\n        # Download the PDF if it exists\n        _, title, url = pdf_info\n        print(f\"SN {sn}: Downloading {filename} - {title}\")\n        success = download_pdf(url, filename)\n        if not success:\n            # Create placeholder if download failed\n            print(f\"SN {sn}: Creating placeholder for {filename} (download failed)\")\n            create_placeholder_pdf(filename, f\"{year} Week {week}\")\n    else:\n        # Create a placeholder PDF if missing\n        print(f\"SN {sn}: Creating placeholder for {filename} (missing)\")\n        create_placeholder_pdf(filename, f\"{year} Week {week}\")\n\n    if week >= weeks_in_year:\n        year += 1\n        week -= weeks_in_year\n    week += 1\n\nprint(f\"\\nProcessing complete!\")\nprint(f\"Files saved to: {SAVE_DIR}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-02T10:33:42.907306Z","iopub.execute_input":"2025-12-02T10:33:42.907735Z","iopub.status.idle":"2025-12-02T10:33:47.915183Z","shell.execute_reply.started":"2025-12-02T10:33:42.907707Z","shell.execute_reply":"2025-12-02T10:33:47.914076Z"}},"outputs":[{"name":"stdout","text":"SN range in data: 1 to 443\n\nProcessing SN range: 307 to 1\nSN 307: Downloading 2020-W01.pdf - An update of Lassa fever outbreak in Nigeria for Week 1\n✓ Downloaded: 2020-W01.pdf\nSN 306: Downloading 2020-W02.pdf - An update of Lassa fever outbreak in Nigeria for Week 2\n✓ Downloaded: 2020-W02.pdf\nSN 305: Downloading 2020-W03.pdf - An update of Lassa fever outbreak in Nigeria for Week 3\n✓ Downloaded: 2020-W03.pdf\nSN 304: Downloading 2020-W04.pdf - An update of Lassa fever outbreak in Nigeria for Week 4\n✓ Downloaded: 2020-W04.pdf\nSN 303: Downloading 2020-W05.pdf - An update of Lassa fever outbreak in Nigeria for Week 5\n✓ Downloaded: 2020-W05.pdf\nSN 302: Downloading 2020-W06.pdf - An update of Lassa fever outbreak in Nigeria for Week 6\n✓ Downloaded: 2020-W06.pdf\nSN 301: Downloading 2020-W07.pdf - An update of Lassa fever outbreak in Nigeria for Week 7\n✓ Downloaded: 2020-W07.pdf\nSN 300: Downloading 2020-W08.pdf - An update of Lassa fever outbreak in Nigeria for Week 8\n✓ Downloaded: 2020-W08.pdf\nSN 299: Downloading 2020-W09.pdf - An update of Lassa fever outbreak in Nigeria for Week 9\n✓ Downloaded: 2020-W09.pdf\nSN 298: Downloading 2020-W10.pdf - An update of Lassa fever outbreak in Nigeria for Week 10\n✓ Downloaded: 2020-W10.pdf\n\nProcessing complete!\nFiles saved to: /kaggle/working/ncdc_reports\n","output_type":"stream"}],"execution_count":11},{"cell_type":"markdown","source":"## PDF Data Extraction","metadata":{}},{"cell_type":"markdown","source":"### Helper Function Definition","metadata":{}},{"cell_type":"code","source":"# Helper functions \ndef get_week_year_from_sn(sn: int) -> tuple[int, int]:\n    \"\"\"\n    Calculate ISO year and week number from serial number (SN).\n    \n    Args:\n        sn (int): Serial number (week count)\n    \n    Returns:\n        tuple[int, int]: (year, week) corresponding to the SN\n    \"\"\"\n    weeks_passed = sn - START_SN  # difference from starting SN\n    current_date = dt.datetime(START_YEAR, 1, 1)\n    \n    # Find the first Thursday of the year (ISO week rule)\n    while current_date.weekday() != 3:  # 3 = Thursday\n        current_date += dt.timedelta(days=1)\n    \n    # Add weeks passed to get target date\n    target_date = current_date + dt.timedelta(weeks=weeks_passed)\n    year = target_date.isocalendar()[0]\n    week = target_date.isocalendar()[1]\n    \n    return year, week\n\n\ndef get_week_start_and_end_date(year: int, week: int) -> tuple[dt.datetime, dt.datetime]:\n    \"\"\"\n    Calculate the start (Monday) and end (Sunday) dates of a given ISO week.\n    \n    Args:\n        year (int): ISO year\n        week (int): ISO week number\n    \n    Returns:\n        tuple(datetime, datetime): (week_start, week_end)\n    \"\"\"\n    # Get first day of the year\n    first_jan = dt.datetime(year, 1, 1)\n    \n    # Find the first Thursday of the year (ISO week 1 contains Jan 4th)\n    while first_jan.weekday() != 3:  # 3 = Thursday\n        first_jan += dt.timedelta(days=1)\n    \n    # Calculate Monday of the ISO week\n    week_start = first_jan + dt.timedelta(weeks=week-1) - dt.timedelta(days=3)\n    week_end = week_start + dt.timedelta(days=6)  # Sunday of the same week\n    \n    return week_start, week_end\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-02T10:33:47.916129Z","iopub.execute_input":"2025-12-02T10:33:47.916374Z","iopub.status.idle":"2025-12-02T10:33:47.925521Z","shell.execute_reply.started":"2025-12-02T10:33:47.916353Z","shell.execute_reply":"2025-12-02T10:33:47.924308Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"def extract_table_from_pdf(pdf_path: str) -> list:\n    \"\"\"\n    Extract tables from the first page of a PDF and return their content.\n    \n    Args:\n        pdf_path (str): Path to the PDF file.\n    \n    Returns:\n        list: Extracted tables as lists of rows (each row is a list of cell values).\n    \"\"\"\n    with pdfplumber.open(pdf_path) as pdf:\n        page = pdf.pages[0]  # use first page\n        \n        # Settings to guide table extraction\n        table_settings = {\n            \"vertical_strategy\": \"lines\",         # use vertical lines for columns\n            \"horizontal_strategy\": \"lines\",       # use horizontal lines for rows\n            \"explicit_vertical_lines\": [],        # no manually specified vertical lines\n            \"explicit_horizontal_lines\": [],      # no manually specified horizontal lines\n            \"snap_tolerance\": 3,                  # tolerance for snapping edges\n            \"join_tolerance\": 3,                  # tolerance for joining adjacent lines\n            \"edge_min_length\": 3,                 # minimum line length to consider\n            \"min_words_vertical\": 2,              # min words to form vertical segment\n            \"min_words_horizontal\": 1,            # min words to form horizontal segment\n        }\n        \n        tables = page.extract_tables(table_settings)  # extract tables\n        return tables","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-02T10:33:47.926981Z","iopub.execute_input":"2025-12-02T10:33:47.927259Z","iopub.status.idle":"2025-12-02T10:33:47.953953Z","shell.execute_reply.started":"2025-12-02T10:33:47.927233Z","shell.execute_reply":"2025-12-02T10:33:47.952714Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"def extract_case_data_from_table(data: list) -> Optional[Tuple[Any, Any, Any, Any]]:\n    \"\"\"\n    Extract suspected, confirmed, probable cases, and deaths from a table.\n    Searches for the row containing 'Current week' or 'Currentweek' and\n    handles different table layouts.\n    \n    Args:\n        data (list): Nested list representing the PDF table content.\n    \n    Returns:\n        tuple or None: (suspected, confirmed, probable, deaths) if found, else None.\n    \"\"\"\n\n    # --- recursive search for row containing \"Current week\" ---\n    def search(lst: list) -> Optional[list]:\n        for item in lst:\n            if isinstance(item, list):\n                found = search(item)\n                if found:\n                    return found\n            elif isinstance(item, str) and (\"Current week\" in item or \"Currentweek\" in item):\n                return lst\n        return None\n\n    row = search(data)\n    \n    # fallback for uncommon table patterns\n    if not row:\n        if len(data) == 5:\n            try:\n                section = data[3][6]\n                return section[0], section[3], section[6], section[9]\n            except Exception:\n                return None\n        return None\n\n    # standard layout (row has consecutive values)\n    if len(row) >= 5 and row[1] is not None:\n        try:\n            return row[1], row[2], row[3], row[4]\n        except Exception:\n            pass\n\n    # sparse layout (values spread out across row)\n    if len(row) >= 13:\n        try:\n            return row[3], row[6], row[9], row[12]\n        except Exception:\n            pass\n\n    return None  # no valid data found","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-02T10:33:47.955133Z","iopub.execute_input":"2025-12-02T10:33:47.955421Z","iopub.status.idle":"2025-12-02T10:33:47.981581Z","shell.execute_reply.started":"2025-12-02T10:33:47.955393Z","shell.execute_reply":"2025-12-02T10:33:47.980636Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"def extract_epi_week(pdf_path: str) -> Optional[str]:\n    \"\"\"\n    Extract the Epi Week line from a PDF.\n    Handles multiple formats by using regex patterns and checks first few pages.\n\n    Args:\n        pdf_path (str): Path to the PDF file.\n\n    Returns:\n        str or None: Extracted Epi Week string if found, else None.\n    \"\"\"\n    # Patterns to match different Epi Week formats\n    patterns = [\n        r\"EpiWeek\\s*\\d+:\\s*\\d+[–-]\\d+[A-Za-z]+\\d{4}\",                        # Compressed format: \"EpiWeek20:17–23May2021\"\n        r\"Epi Week\\s*\\d+:\\s*\\d+(?:st|nd|rd|th)?\\s*[–-]\\s*\\d+(?:st|nd|rd|th)?\\s*[A-Za-z]+\\s*\\d{4}\",  # e.g., \"Epi Week 30: 22nd – 28th July 2024\"\n        r\"Epi Week\\s*\\d+:\\s*\\d+\\s*[–-]\\s*\\d+\\s*[A-Za-z]+\\s*\\d{4}\",           # e.g., \"Epi Week 29: 18 – 24 July 2022\"\n        r\"Epi Week\\s*\\d+[^\\n]{0,100}\\d{4}\",                                  # More general\n        r\"Epi Week[^\\n]{0,150}\",                                             # Even more general\n    ]\n    \n    try:\n        with pdfplumber.open(pdf_path) as pdf:\n            for page_num, page in enumerate(pdf.pages[:3]):  # Check first 3 pages\n                text = page.extract_text()\n                \n                if text:\n                    # Clean text: remove extra whitespace\n                    cleaned_text = ' '.join(text.split())\n                    \n                    # Search for patterns\n                    for pattern in patterns:\n                        match = re.search(pattern, cleaned_text, re.IGNORECASE)\n                        if match:\n                            return match.group().strip()\n                    \n                    # Fallback: line-by-line search for \"Epi Week\"\n                    for line in text.split('\\n'):\n                        if 'Epi Week' in line:\n                            return ' '.join(line.split())  # clean whitespace\n                            \n    except Exception as e:\n        print(f\"Error processing PDF: {e}\")\n        return None\n    \n    return None  # return None if not found\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-02T10:33:47.985548Z","iopub.execute_input":"2025-12-02T10:33:47.985835Z","iopub.status.idle":"2025-12-02T10:33:48.012155Z","shell.execute_reply.started":"2025-12-02T10:33:47.985816Z","shell.execute_reply":"2025-12-02T10:33:48.010916Z"}},"outputs":[],"execution_count":15},{"cell_type":"markdown","source":"### Main","metadata":{}},{"cell_type":"code","source":"def get_data_from_pdf(pdf_path: str) -> Optional[Dict[str, Any]]:\n    \"\"\"\n    Extract epidemiological data from a Lassa Fever PDF report.\n    Retrieves Epi Week, suspected, confirmed, probable cases, and deaths.\n    Handles multiple table formats.\n\n    Args:\n        pdf_path (str): Path to the PDF file.\n\n    Returns:\n        dict or None: Dictionary with keys:\n            - 'epi_week'\n            - 'suspected_cases'\n            - 'confirmed_cases'\n            - 'probable_cases'\n            - 'deaths'\n        Returns None if the PDF is missing or cannot be processed.\n    \"\"\"\n    try:\n        # Extract Epi Week string\n        epi_week = extract_epi_week(pdf_path)\n\n        # Extract table content and parse case numbers\n        table_content = extract_table_from_pdf(pdf_path)\n        tup = extract_case_data_from_table(table_content)\n        \n        # Return structured data as dictionary\n        return {\n            'epi_week': epi_week,\n            'suspected_cases': tup[0],\n            'confirmed_cases': tup[1],\n            'probable_cases': tup[2],\n            'deaths': tup[3]\n        }\n\n    except FileNotFoundError:\n        print(f\"Error: File not found at {pdf_path}\")\n        return None\n    except Exception as e:\n        print(f\"Error processing PDF: {str(e)}\")\n        return None","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-02T10:33:48.013143Z","iopub.execute_input":"2025-12-02T10:33:48.013479Z","iopub.status.idle":"2025-12-02T10:33:48.037611Z","shell.execute_reply.started":"2025-12-02T10:33:48.013445Z","shell.execute_reply":"2025-12-02T10:33:48.036443Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"def process_all_reports(directory_path: str) -> pd.DataFrame:\n    \"\"\"\n    Process all PDF files in a directory and extract epidemiological data\n    for time series analysis.\n\n    Args:\n        directory_path (str): Path to directory containing PDF files.\n\n    Returns:\n        pd.DataFrame: DataFrame with columns including week start/end dates,\n        Epi Week info, cases (suspected, confirmed, probable, deaths),\n        extraction method, and timestamp.\n    \"\"\"\n    data_records = []\n\n    # Get all PDF files and sort by filename\n    pdf_files = [f for f in os.listdir(directory_path) if f.endswith('.pdf')]\n    pdf_files.sort()  # ensures chronological order like 2020-W01.pdf, 2020-W02.pdf, etc.\n\n    print(f\"Found {len(pdf_files)} PDF files to process\")\n\n    for filename in pdf_files:\n        # Extract year and week from filename\n        match = re.match(r'(\\d{4})-W(\\d{2})\\.pdf', filename)\n        if not match:\n            print(f\"Skipping invalid filename: {filename}\")\n            continue\n\n        year = int(match.group(1))\n        week = int(match.group(2))\n\n        # Calculate start (Monday) and end (Sunday) dates of the ISO week\n        week_start_date, week_end_date = get_week_start_and_end_date(year, week)\n\n        # Full path to PDF\n        pdf_path = os.path.join(directory_path, filename)\n\n        # Extract data from PDF\n        result = get_data_from_pdf(pdf_path)\n\n        if result:\n            # Successful extraction\n            record = {\n                'week_start_date': week_start_date,\n                'week_end_date': week_end_date,\n                'epi_year': year,\n                'epi_week': week,\n                'filename': filename,\n                'epi_week_info': result['epi_week'],\n                'suspected_cases': result['suspected_cases'],\n                'confirmed_cases': result['confirmed_cases'],\n                'probable_cases': result['probable_cases'],\n                'deaths': result['deaths'],\n                'extraction_method': \"automated\",\n                'extraction_timestamp': dt.datetime.now().isoformat(),\n            }\n            data_records.append(record)\n            print(f\"✓ Processed {filename}: {result['confirmed_cases']} confirmed cases\")\n        else:\n            # For placeholder files or failed extraction\n            record = {\n                'week_start_date': week_start_date,\n                'week_end_date': week_end_date,\n                'epi_year': year,\n                'epi_week': week,\n                'filename': filename,\n                'epi_week_info': None,\n                'suspected_cases': None,\n                'confirmed_cases': None,\n                'probable_cases': None,\n                'deaths': None,\n                'extraction_method': \"manual\",\n                'extraction_timestamp': None,\n            }\n            data_records.append(record)\n            print(f\"✗ Failed to extract data from {filename}\")\n\n    # Create DataFrame\n    df = pd.DataFrame(data_records)\n\n    # Sort by week start date to ensure chronological order\n    df = df.sort_values('week_start_date').reset_index(drop=True)\n\n    return df\n\n\n# ------------------ Main execution ------------------\nif __name__ == \"__main__\":\n    directory_path = \"/kaggle/working/ncdc_reports\"\n\n    # Process all reports in the directory\n    final_df = process_all_reports(directory_path)\n\n    # Display summary\n    print(\"\\n\" + \"=\"*50)\n    print(\"EXTRACTED DATA SUMMARY\")\n    print(\"=\"*50)\n    print(f\"Total records: {len(final_df)}\")\n    print(f\"Records with case data: {final_df['confirmed_cases'].notna().sum()}\")\n    print(f\"Date range: {final_df['week_start_date'].min()} to {final_df['week_start_date'].max()}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-02T10:33:48.038955Z","iopub.execute_input":"2025-12-02T10:33:48.039868Z","iopub.status.idle":"2025-12-02T10:33:52.83739Z","shell.execute_reply.started":"2025-12-02T10:33:48.039833Z","shell.execute_reply":"2025-12-02T10:33:52.836199Z"}},"outputs":[{"name":"stdout","text":"Found 10 PDF files to process\n✓ Processed 2020-W01.pdf:  confirmed cases\n✓ Processed 2020-W02.pdf:  confirmed cases\n✓ Processed 2020-W03.pdf:  confirmed cases\n✓ Processed 2020-W04.pdf: 95 confirmed cases\n✓ Processed 2020-W05.pdf: 104 confirmed cases\n✓ Processed 2020-W06.pdf: 109 confirmed cases\n✓ Processed 2020-W07.pdf: 115 confirmed cases\n✓ Processed 2020-W08.pdf: 102 confirmed cases\n✓ Processed 2020-W09.pdf: 85 confirmed cases\n✓ Processed 2020-W10.pdf: 81 confirmed cases\n\n==================================================\nEXTRACTED DATA SUMMARY\n==================================================\nTotal records: 10\nRecords with case data: 10\nDate range: 2019-12-30 00:00:00 to 2020-03-02 00:00:00\n","output_type":"stream"}],"execution_count":17},{"cell_type":"markdown","source":"### Export / Save Processed Data","metadata":{}},{"cell_type":"code","source":"# Add PDF URLs to final DataFrame \nurls = []\n\n# Start from START_SN - 1 to match the first PDF in pdf_data\nindex = START_SN - 1\n\nfor sn in range(len(final_df)):\n    appropriate_url = pdf_data[index][2]\n    urls.append(appropriate_url)\n\n    index -= 1 \n    \nfinal_df[\"report_pdf_url\"] = urls","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-02T10:33:52.838601Z","iopub.execute_input":"2025-12-02T10:33:52.838852Z","iopub.status.idle":"2025-12-02T10:33:52.846316Z","shell.execute_reply.started":"2025-12-02T10:33:52.838832Z","shell.execute_reply":"2025-12-02T10:33:52.844646Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"# Save to CSV\ncsv_path = \"/kaggle/working/lassa_fever_timeseries.csv\"\nfinal_df.to_csv(csv_path, index=False)\nprint(f\"\\nData saved to: {csv_path}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-02T10:33:52.847681Z","iopub.execute_input":"2025-12-02T10:33:52.848046Z","iopub.status.idle":"2025-12-02T10:33:52.876883Z","shell.execute_reply.started":"2025-12-02T10:33:52.848016Z","shell.execute_reply":"2025-12-02T10:33:52.87571Z"}},"outputs":[{"name":"stdout","text":"\nData saved to: /kaggle/working/lassa_fever_timeseries.csv\n","output_type":"stream"}],"execution_count":19},{"cell_type":"markdown","source":"## Cleanup / Additional Formatting","metadata":{}},{"cell_type":"code","source":"# create zip file \n\ndirectory_path = \"/kaggle/working/ncdc_reports\"\nzip_filename = \"/kaggle/working/ncdc_reports.zip\"\n\n# Create zip file\nshutil.make_archive(\n    base_name=\"/kaggle/working/ncdc_reports\", \n    format='zip',\n    root_dir=directory_path\n)\n\nprint(f\"Zip file created: {zip_filename}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-02T10:33:52.878013Z","iopub.execute_input":"2025-12-02T10:33:52.878383Z","iopub.status.idle":"2025-12-02T10:33:53.57952Z","shell.execute_reply.started":"2025-12-02T10:33:52.878353Z","shell.execute_reply":"2025-12-02T10:33:53.578672Z"}},"outputs":[{"name":"stdout","text":"Zip file created: /kaggle/working/ncdc_reports.zip\n","output_type":"stream"}],"execution_count":20},{"cell_type":"code","source":"# delete all files \nimport os\nimport glob\n\ndirectory_path = \"/kaggle/working/ncdc_reports\"\n\nfiles = glob.glob(os.path.join(directory_path, \"*\"))\nfor file in files:\n    if os.path.isfile(file):\n        os.remove(file)\n        print(f\"Deleted: {file}\")\n\nprint(\"All files deleted!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-02T10:33:53.580698Z","iopub.execute_input":"2025-12-02T10:33:53.581045Z","iopub.status.idle":"2025-12-02T10:33:53.590937Z","shell.execute_reply.started":"2025-12-02T10:33:53.581022Z","shell.execute_reply":"2025-12-02T10:33:53.589979Z"}},"outputs":[{"name":"stdout","text":"Deleted: /kaggle/working/ncdc_reports/2020-W06.pdf\nDeleted: /kaggle/working/ncdc_reports/2020-W04.pdf\nDeleted: /kaggle/working/ncdc_reports/2020-W05.pdf\nDeleted: /kaggle/working/ncdc_reports/2020-W03.pdf\nDeleted: /kaggle/working/ncdc_reports/2020-W09.pdf\nDeleted: /kaggle/working/ncdc_reports/2020-W07.pdf\nDeleted: /kaggle/working/ncdc_reports/2020-W02.pdf\nDeleted: /kaggle/working/ncdc_reports/2020-W01.pdf\nDeleted: /kaggle/working/ncdc_reports/2020-W10.pdf\nDeleted: /kaggle/working/ncdc_reports/2020-W08.pdf\nAll files deleted!\n","output_type":"stream"}],"execution_count":21},{"cell_type":"code","source":"from IPython.display import FileLink\nFileLink('ncdc_reports.zip')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-02T10:33:53.592042Z","iopub.execute_input":"2025-12-02T10:33:53.592337Z","iopub.status.idle":"2025-12-02T10:33:53.609875Z","shell.execute_reply.started":"2025-12-02T10:33:53.59231Z","shell.execute_reply":"2025-12-02T10:33:53.608656Z"}},"outputs":[{"execution_count":22,"output_type":"execute_result","data":{"text/plain":"/kaggle/working/ncdc_reports.zip","text/html":"<a href='ncdc_reports.zip' target='_blank'>ncdc_reports.zip</a><br>"},"metadata":{}}],"execution_count":22}]}